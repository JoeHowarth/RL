{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-a3602276574d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-c7498d980ef5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_replay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mdisplay_screen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-1fbc6cfd01c9>\u001b[0m in \u001b[0;36mdisplay_screen\u001b[0;34m(action, points, input_t)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#     print (\"Action %s, Points: %d\" % (translate_action[action],points))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"End\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtranslate_action\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         plt.imshow(input_t.reshape((grid_size,)*2),\n\u001b[1;32m     23\u001b[0m                interpolation='none', cmap='gray')\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPQAAAD3CAYAAAAqu3lQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c771e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn\n",
    "seaborn.set()\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from keras.models import model_from_json\n",
    "from qlearn import Catch\n",
    "from PIL import Image\n",
    "from IPython import display\n",
    "last_frame_time = 0\n",
    "translate_action = [\"Left\",\"Stay\",\"Right\",\"Create Ball\",\"End Test\"]\n",
    "grid_size = 10\n",
    "\n",
    "def display_screen(action,points,input_t):\n",
    "    print(\"action\", action, type(action))\n",
    "    global last_frame_time\n",
    "    display.clear_output(wait=True)\n",
    "#     print (\"Action %s, Points: %d\" % (translate_action[action],points))\n",
    "    if(\"End\" not in translate_action[action]):\n",
    "        plt.imshow(input_t.reshape((grid_size,)*2),\n",
    "               interpolation='none', cmap='gray')\n",
    "        display.display(plt.gcf())\n",
    "    last_frame_time = set_max_fps(last_frame_time)\n",
    "def set_max_fps(last_frame_time,FPS = 1):\n",
    "    current_milli_time = lambda: int(round(time.time() * 1000))\n",
    "    sleep_time = 1./FPS - (current_milli_time() - last_frame_time)\n",
    "    if sleep_time > 0:\n",
    "        time.sleep(sleep_time)\n",
    "    return current_milli_time()\n",
    "def test(model):\n",
    "    global last_frame_time\n",
    "    plt.ion()\n",
    "    # Define environment, game\n",
    "    env = Catch(grid_size)\n",
    "    c = 0\n",
    "    last_frame_time = 0\n",
    "    points = 0\n",
    "    for e in range(10):\n",
    "        loss = 0.\n",
    "        env.reset()\n",
    "        game_over = False\n",
    "        # get initial input\n",
    "        input_t = env.observe()\n",
    "        display_screen(3,points,input_t)\n",
    "        c += 1\n",
    "        while not game_over:\n",
    "            input_tm1 = input_t\n",
    "            # get next action\n",
    "            q = model.predict(input_tm1)\n",
    "            action = np.argmax(q[0])\n",
    "            # apply action, get rewards and new state\n",
    "            input_t, reward, game_over = env.act(action)\n",
    "            points += reward\n",
    "            display_screen(action,points,input_t)\n",
    "            c += 1\n",
    "    display_screen(4,points,input_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import sgd\n",
    "\n",
    "\n",
    "# parameters\n",
    "epsilon = .1  # exploration\n",
    "num_actions = 3  # [move_left, stay, move_right]\n",
    "epoch = 100\n",
    "max_memory = 500\n",
    "hidden_size = 100\n",
    "batch_size = 1\n",
    "grid_size = 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(hidden_size, input_shape=(grid_size**2,), activation='relu'))\n",
    "model.add(Dense(hidden_size, activation='relu'))\n",
    "model.add(Dense(num_actions))\n",
    "model.compile(sgd(lr=.2), \"mse\")\n",
    "    \n",
    "# If you want to continue training from a previous model, just uncomment the line bellow\n",
    "# model.load_weights(\"model.h5\")\n",
    "\n",
    "# Define environment/game\n",
    "env = Catch(grid_size)\n",
    "\n",
    "# Initialize experience replay object\n",
    "exp_replay = ExperienceReplay(max_memory=max_memory)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ExperienceReplay(object):\n",
    "    def __init__(self, max_memory=100, discount=.9):\n",
    "        self.max_memory = max_memory\n",
    "        self.memory = list()\n",
    "        self.discount = discount\n",
    "\n",
    "    def remember(self, states, game_over):\n",
    "        # memory[i] = [[state_t, action_t, reward_t, state_t+1], game_over?]\n",
    "        self.memory.append([states, game_over])\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def get_batch(self, model, batch_size=10):\n",
    "        len_memory = len(self.memory)\n",
    "        num_actions = model.output_shape[-1]\n",
    "        env_dim = self.memory[0][0][0].shape[1]\n",
    "        inputs = np.zeros((min(len_memory, batch_size), env_dim))\n",
    "        targets = np.zeros((inputs.shape[0], num_actions))\n",
    "        for i, idx in enumerate(np.random.randint(0, len_memory,\n",
    "                                                  size=inputs.shape[0])):\n",
    "            state_t, action_t, reward_t, state_tp1 = self.memory[idx][0]\n",
    "            game_over = self.memory[idx][1]\n",
    "\n",
    "            inputs[i:i+1] = state_t\n",
    "            # There should be no target values for actions not taken.\n",
    "            # Thou shalt not correct actions not taken #deep\n",
    "            targets[i] = model.predict(state_t)[0]\n",
    "            Q_sa = np.max(model.predict(state_tp1)[0])\n",
    "            if game_over:  # if game_over is True\n",
    "                targets[i, action_t] = reward_t\n",
    "            else:\n",
    "                # reward_t + gamma * max_a' Q(s', a')\n",
    "                targets[i, action_t] = reward_t + self.discount * Q_sa\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Catch(object):\n",
    "    def __init__(self, grid_size=10):\n",
    "        self.grid_size = grid_size\n",
    "        self.reset()\n",
    "\n",
    "    def _update_state(self, action):\n",
    "        \"\"\"\n",
    "        Input: action and states\n",
    "        Ouput: new states and reward\n",
    "        \"\"\"\n",
    "        state = self.state\n",
    "        if action == 0:  # left\n",
    "            action = -1\n",
    "        elif action == 1:  # stay\n",
    "            action = 0\n",
    "        else:\n",
    "            action = 1  # right\n",
    "        f0, f1, basket = state[0]\n",
    "        new_basket = min(max(1, basket + action), self.grid_size-1)\n",
    "        f0 += 1\n",
    "        out = np.asarray([f0, f1, new_basket])\n",
    "        out = out[np.newaxis]\n",
    "\n",
    "        assert len(out.shape) == 2\n",
    "        self.state = out\n",
    "\n",
    "    def _draw_state(self):\n",
    "        im_size = (self.grid_size,)*2\n",
    "        state = self.state[0]\n",
    "        canvas = np.zeros(im_size)\n",
    "        canvas[state[0], state[1]] = 1  # draw fruit\n",
    "        canvas[-1, state[2]-1:state[2] + 2] = 1  # draw basket\n",
    "        return canvas\n",
    "        \n",
    "    def _get_reward(self):\n",
    "        fruit_row, fruit_col, basket = self.state[0]\n",
    "        if fruit_row == self.grid_size-1:\n",
    "            if abs(fruit_col - basket) <= 1:\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def _is_over(self):\n",
    "        if self.state[0, 0] == self.grid_size-1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def observe(self):\n",
    "        canvas = self._draw_state()\n",
    "        return canvas.reshape((1, -1))\n",
    "\n",
    "    def act(self, action):\n",
    "        self._update_state(action)\n",
    "        reward = self._get_reward()\n",
    "        game_over = self._is_over()\n",
    "        return self.observe(), reward, game_over\n",
    "\n",
    "    def reset(self):\n",
    "        n = np.random.randint(0, self.grid_size-1, size=1)\n",
    "        m = np.random.randint(1, self.grid_size-2, size=1)\n",
    "        self.state = np.asarray([0, n, m])[np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    # Train\n",
    "    win_cnt = 0\n",
    "    for e in range(1):\n",
    "        loss = 0.\n",
    "        env.reset()\n",
    "        game_over = False\n",
    "        # get initial input\n",
    "        input_t = env.observe()\n",
    "\n",
    "        while not game_over:\n",
    "            input_tm1 = input_t\n",
    "            # get next action\n",
    "            if np.random.rand() <= epsilon:\n",
    "                action = np.random.randint(0, num_actions, size=1)\n",
    "            else:\n",
    "                q = model.predict(input_tm1)\n",
    "                action = np.argmax(q[0])\n",
    "\n",
    "            # apply action, get rewards and new state\n",
    "            input_t, reward, game_over = env.act(action)\n",
    "            if reward == 1:\n",
    "                win_cnt += 1\n",
    "\n",
    "            # store experience\n",
    "            exp_replay.remember([input_tm1, action, reward, input_t], game_over)            \n",
    "            \n",
    "            # adapt model\n",
    "            inputs, targets = exp_replay.get_batch(model, batch_size=batch_size)\n",
    "            \n",
    "#             display_screen(action,3000,inputs[0])            \n",
    "            \n",
    "            loss += model.train_on_batch(inputs, targets)\n",
    "        print(\"Epoch {:03d}/999 | Loss {:.4f} | Win count {}\".format(e, loss, win_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
